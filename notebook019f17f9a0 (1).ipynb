{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11223020,"sourceType":"datasetVersion","datasetId":7009118}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade peft transformers accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U bitsandbytes ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import get_scheduler\n\nimport os\n\n\n\n# Configuration\nBATCH_SIZE = 8\nLEARNING_RATE = 2e-4\nWEIGHT_DECAY = 0.01\nNUM_EPOCHS = 4\nWARMUP_STEPS = 100\nEVAL_STEPS = 200\nGRADIENT_ACCUMULATION_STEPS = 4\nMAX_LENGTH = 50\nDATASET_SIZE = 5000 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:29:55.128958Z","iopub.execute_input":"2025-04-05T10:29:55.129266Z","iopub.status.idle":"2025-04-05T10:30:11.624513Z","shell.execute_reply.started":"2025-04-05T10:29:55.129244Z","shell.execute_reply":"2025-04-05T10:30:11.623793Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"ds = load_dataset(\"tomytjandra/h-and-m-fashion-caption-12k\", split='train')\nds_small = ds.select(range(DATASET_SIZE))\nprint(f\"Dataset size: {len(ds_small)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:30:25.098552Z","iopub.execute_input":"2025-04-05T10:30:25.099343Z","iopub.status.idle":"2025-04-05T10:30:27.554413Z","shell.execute_reply.started":"2025-04-05T10:30:25.099308Z","shell.execute_reply":"2025-04-05T10:30:27.553596Z"}},"outputs":[{"name":"stdout","text":"Dataset size: 5000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"ds_small = ds.select(range(2500))\nprint(f\"Dataset size: {len(ds_small)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:30:32.647651Z","iopub.execute_input":"2025-04-05T10:30:32.648017Z","iopub.status.idle":"2025-04-05T10:30:32.656668Z","shell.execute_reply.started":"2025-04-05T10:30:32.647994Z","shell.execute_reply":"2025-04-05T10:30:32.655789Z"}},"outputs":[{"name":"stdout","text":"Dataset size: 2500\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"ds_small","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:29:26.828209Z","iopub.execute_input":"2025-04-05T10:29:26.828522Z","iopub.status.idle":"2025-04-05T10:29:26.837983Z","shell.execute_reply.started":"2025-04-05T10:29:26.828500Z","shell.execute_reply":"2025-04-05T10:29:26.836783Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-4e70ece4eb27>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds_small\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'ds_small' is not defined"],"ename":"NameError","evalue":"name 'ds_small' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"ds_small = ds_small.shuffle(seed=42)\ntrain_size = int(0.9 * len(ds_small))\ntrain_dataset_raw = ds_small.select(range(train_size))\neval_dataset_raw = ds_small.select(range(train_size, len(ds_small)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:03:47.184615Z","iopub.execute_input":"2025-04-05T09:03:47.184989Z","iopub.status.idle":"2025-04-05T09:03:47.200579Z","shell.execute_reply.started":"2025-04-05T09:03:47.184962Z","shell.execute_reply":"2025-04-05T09:03:47.199716Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:03:56.827164Z","iopub.execute_input":"2025-04-05T09:03:56.827521Z","iopub.status.idle":"2025-04-05T09:03:56.833431Z","shell.execute_reply.started":"2025-04-05T09:03:56.827489Z","shell.execute_reply":"2025-04-05T09:03:56.832418Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"2250"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"class ImageCaptioningDataset(Dataset):\n    def __init__(self, dataset, processor, max_length=MAX_LENGTH):\n        self.dataset = dataset\n        self.processor = processor\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        encoding = self.processor(\n            images=item[\"image\"], \n            text=item[\"text\"],\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        # Remove batch dimension\n        encoding = {k: v.squeeze() for k, v in encoding.items()}\n        return encoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:04:07.947050Z","iopub.execute_input":"2025-04-05T09:04:07.947414Z","iopub.status.idle":"2025-04-05T09:04:07.953487Z","shell.execute_reply.started":"2025-04-05T09:04:07.947384Z","shell.execute_reply":"2025-04-05T09:04:07.952405Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def collate_fn(batch):\n    processed_batch = {}\n    \n    # Process image inputs\n    processed_batch[\"pixel_values\"] = torch.stack([example[\"pixel_values\"] for example in batch])\n    \n    # Process text inputs (labels)\n    input_ids = [example[\"input_ids\"] for example in batch]\n    attention_mask = [example[\"attention_mask\"] for example in batch]\n    \n    # Pad the input_ids and attention_mask\n    max_length = max(len(ids) for ids in input_ids)\n    \n    # Create padded tensors\n    padded_input_ids = torch.full((len(batch), max_length), processor.tokenizer.pad_token_id, dtype=torch.long)\n    padded_attention_mask = torch.zeros((len(batch), max_length), dtype=torch.long)\n    \n    for i, (ids, mask) in enumerate(zip(input_ids, attention_mask)):\n        length = len(ids)\n        padded_input_ids[i, :length] = ids\n        padded_attention_mask[i, :length] = mask\n    \n    processed_batch[\"input_ids\"] = padded_input_ids\n    processed_batch[\"attention_mask\"] = padded_attention_mask\n    \n    # Set labels for calculating loss\n    processed_batch[\"labels\"] = processed_batch[\"input_ids\"].clone()\n    \n    return processed_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:04:11.963965Z","iopub.execute_input":"2025-04-05T09:04:11.964273Z","iopub.status.idle":"2025-04-05T09:04:11.971507Z","shell.execute_reply.started":"2025-04-05T09:04:11.964250Z","shell.execute_reply":"2025-04-05T09:04:11.970539Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:04:24.564508Z","iopub.execute_input":"2025-04-05T09:04:24.565057Z","iopub.status.idle":"2025-04-05T09:04:24.640457Z","shell.execute_reply.started":"2025-04-05T09:04:24.565009Z","shell.execute_reply":"2025-04-05T09:04:24.639307Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(\n    load_in_4bit=True, \n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:04:28.166042Z","iopub.execute_input":"2025-04-05T09:04:28.166497Z","iopub.status.idle":"2025-04-05T09:04:28.173361Z","shell.execute_reply.started":"2025-04-05T09:04:28.166453Z","shell.execute_reply":"2025-04-05T09:04:28.172343Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-opt-2.7b\", \n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:04:30.563439Z","iopub.execute_input":"2025-04-05T09:04:30.563811Z","iopub.status.idle":"2025-04-05T09:06:58.946451Z","shell.execute_reply.started":"2025-04-05T09:04:30.563780Z","shell.execute_reply":"2025-04-05T09:06:58.945676Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b732571640184485a3613634341617ea"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b1ab945229a43f19a524e7cb32c9a95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/882 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ffd3f9c794141c98e305bd2d4d59a8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39898ad04cb64ea19f86c3b5715e9539"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f890228164e474a82e11788de2fc411"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.56M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7287d7405d7a41bab93e620179fdc9f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f0f39acbc04ec9a01f4dbc16887ab0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/548 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd05420496134897811ae508128ffee1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b79107948e02494f86ccd0f311aec81b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/122k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2abf68f78fbd410da12ba2d92f4efdbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fdfe3687317441196576d211b7b50cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc9d74cda4704d7986044b9b1bdc38bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ece5cb4e71843b4bd38dd775ac61826"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c4ef7f0d07c4cef8c364af3790c8424"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f3257eebb77410795ee5e65ac600fd1"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]  # Added v_proj and o_proj\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:07:07.563992Z","iopub.execute_input":"2025-04-05T09:07:07.564311Z","iopub.status.idle":"2025-04-05T09:07:07.608929Z","shell.execute_reply.started":"2025-04-05T09:07:07.564286Z","shell.execute_reply":"2025-04-05T09:07:07.607867Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Use PeftModelForSeq2SeqLM instead of the default\nfrom peft import get_peft_model, PeftModelForSeq2SeqLM\n\n# Option 1: Let PEFT determine the model type\nmodel = get_peft_model(model, lora_config)\n\n# Option 2: Explicitly use Seq2Seq model type\nmodel = PeftModelForSeq2SeqLM(model, lora_config)\nmodel.print_trainable_parameters()\n\n# Create datasets and dataloaders\ntrain_dataset = ImageCaptioningDataset(train_dataset_raw, processor)\neval_dataset = ImageCaptioningDataset(eval_dataset_raw, processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:07:11.945659Z","iopub.execute_input":"2025-04-05T09:07:11.946030Z","iopub.status.idle":"2025-04-05T09:07:12.392294Z","shell.execute_reply.started":"2025-04-05T09:07:11.946001Z","shell.execute_reply":"2025-04-05T09:07:12.391337Z"}},"outputs":[{"name":"stdout","text":"trainable params: 7,864,320 || all params: 3,752,626,176 || trainable%: 0.2096\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    train_dataset, \n    shuffle=True, \n    batch_size=BATCH_SIZE, \n    collate_fn=collate_fn\n)\n\neval_dataloader = DataLoader(\n    eval_dataset,\n    batch_size=BATCH_SIZE,\n    collate_fn=collate_fn\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:07:18.085366Z","iopub.execute_input":"2025-04-05T09:07:18.085751Z","iopub.status.idle":"2025-04-05T09:07:18.090626Z","shell.execute_reply.started":"2025-04-05T09:07:18.085721Z","shell.execute_reply":"2025-04-05T09:07:18.089498Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=LEARNING_RATE,\n    weight_decay=WEIGHT_DECAY\n)\n\n# Learning rate scheduler\nnum_training_steps = NUM_EPOCHS * len(train_dataloader)\nlr_scheduler = get_scheduler(\n    \"cosine\",\n    optimizer=optimizer,\n    num_warmup_steps=WARMUP_STEPS,\n    num_training_steps=num_training_steps\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:07:20.764163Z","iopub.execute_input":"2025-04-05T09:07:20.764668Z","iopub.status.idle":"2025-04-05T09:07:20.776671Z","shell.execute_reply.started":"2025-04-05T09:07:20.764627Z","shell.execute_reply":"2025-04-05T09:07:20.775701Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"len(train_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:30:54.709607Z","iopub.execute_input":"2025-04-05T10:30:54.709988Z","iopub.status.idle":"2025-04-05T10:30:55.007846Z","shell.execute_reply.started":"2025-04-05T10:30:54.709963Z","shell.execute_reply":"2025-04-05T10:30:55.006693Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-cf295e9ecab4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"],"ename":"NameError","evalue":"name 'train_dataloader' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"num_training_steps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:07:25.283886Z","iopub.execute_input":"2025-04-05T09:07:25.284364Z","iopub.status.idle":"2025-04-05T09:07:25.290992Z","shell.execute_reply.started":"2025-04-05T09:07:25.284328Z","shell.execute_reply":"2025-04-05T09:07:25.289970Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"1128"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"model.train()\nglobal_step = 0\nbest_eval_loss = float('inf')\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\nEpoch: {epoch+1}/{NUM_EPOCHS}\")\n    \n    # Training\n    model.train()\n    total_loss = 0\n    \n    for batch_idx, batch in enumerate(train_dataloader):\n        # Move batch to device\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        pixel_values = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        # Forward pass - Remove inputs_embeds which is causing the error\n        outputs = model.base_model.forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values,\n            labels=labels,\n            return_dict=True\n        )\n        \n        # Calculate loss\n        loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS\n        total_loss += loss.item()\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights every GRADIENT_ACCUMULATION_STEPS\n        if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            \n            # Log metrics\n            if (global_step + 1) % 10 == 0:\n                avg_loss = total_loss * GRADIENT_ACCUMULATION_STEPS / 10\n                print(f\"Step {global_step+1}/{num_training_steps}, Loss: {avg_loss:.4f}, LR: {lr_scheduler.get_last_lr()[0]:.8f}\")\n                total_loss = 0\n            \n            global_step += 1\n        \n        # Evaluation\n        if global_step > 0 and global_step % EVAL_STEPS == 0:\n            print(\"\\nRunning evaluation...\")\n            model.eval()\n            eval_loss = 0\n            eval_steps = 0\n            \n            with torch.no_grad():\n                for eval_batch in eval_dataloader:\n                    # Move batch to device\n                    input_ids = eval_batch[\"input_ids\"].to(device)\n                    attention_mask = eval_batch[\"attention_mask\"].to(device)\n                    pixel_values = eval_batch[\"pixel_values\"].to(device)\n                    labels = eval_batch[\"labels\"].to(device)\n                    \n                    # Forward pass - consistent with the fix above\n                    outputs = model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        pixel_values=pixel_values,\n                        labels=labels,\n                        return_dict=True\n                    )\n                    \n                    eval_loss += outputs.loss.item()\n                    eval_steps += 1\n            \n            avg_eval_loss = eval_loss / eval_steps\n            print(f\"Evaluation Loss: {avg_eval_loss:.4f}\")\n            \n            \n            # Generate example caption\n            if len(eval_dataset) > 0:\n                example = eval_dataset[0]\n                pixel_values = example[\"pixel_values\"].unsqueeze(0).to(device)\n                \n                # Generate with different parameters\n                with torch.no_grad():\n                    # Standard generation\n                    standard_ids = model.generate(\n                        pixel_values=pixel_values,\n                        max_new_tokens=50\n                    )\n                    standard_caption = processor.batch_decode(standard_ids, skip_special_tokens=True)[0]\n                    \n                    # Generation with sampling\n                    sampled_ids = model.generate(\n                        pixel_values=pixel_values,\n                        max_new_tokens=50,\n                        do_sample=True,\n                        top_k=50,\n                        top_p=0.9,\n                        temperature=0.7\n                    )\n                    sampled_caption = processor.batch_decode(sampled_ids, skip_special_tokens=True)[0]\n                \n                print(f\"Example standard caption: {standard_caption}\")\n                print(f\"Example sampled caption: {sampled_caption}\")\n                \n                # Get ground truth\n                example_input_ids = example[\"input_ids\"]\n                gt_caption = processor.tokenizer.decode(example_input_ids, skip_special_tokens=True)\n                print(f\"Ground truth caption: {gt_caption}\")\n                \n                wandb.log({\n                    \"example_standard\": standard_caption,\n                    \"example_sampled\": sampled_caption,\n                    \"example_ground_truth\": gt_caption\n                }, step=global_step)\n            \n            # Save model checkpoint if it's the best so far\n            if avg_eval_loss < best_eval_loss:\n                best_eval_loss = avg_eval_loss\n                print(f\"New best model with eval loss: {best_eval_loss:.4f}\")\n                \n                # Save LoRA weights\n                output_dir = f\"./checkpoints/epoch-{epoch+1}_step-{global_step}_loss-{avg_eval_loss:.4f}\"\n                model.save_pretrained(output_dir)\n                print(f\"Model saved to {output_dir}\")\n            \n            # Back to training mode\n            model.train()\n\nprint(\"\\nTraining complete!\")\n\n# Final model saving\nmodel.save_pretrained(\"./fashion-captioning-final\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.train()\nglobal_step = 0\nbest_eval_loss = float('inf')\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\nEpoch: {epoch+1}/{NUM_EPOCHS}\")\n    \n    # Training\n    model.train()\n    total_loss = 0\n    \n    for batch_idx, batch in enumerate(train_dataloader):\n        # Move batch to device\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        pixel_values = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        # Forward pass\n        outputs = model.base_model.forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values,\n            labels=labels,\n            return_dict=True\n        )\n        \n        # Calculate loss\n        loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS\n        total_loss += loss.item()\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights every GRADIENT_ACCUMULATION_STEPS\n        if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            \n            # Log metrics\n            if (global_step + 1) % 10 == 0:\n                avg_loss = total_loss * GRADIENT_ACCUMULATION_STEPS / 10\n                print(f\"Step {global_step+1}/{num_training_steps}, Loss: {avg_loss:.4f}, LR: {lr_scheduler.get_last_lr()[0]:.8f}\")\n                total_loss = 0\n            \n            global_step += 1\n        \n        # Evaluation\n        if global_step > 0 and global_step % EVAL_STEPS == 0:\n            print(\"\\nRunning evaluation...\")\n            model.eval()\n            eval_loss = 0\n            eval_steps = 0\n            \n            with torch.no_grad():\n                for eval_batch in eval_dataloader:\n                    # Move batch to device\n                    input_ids = eval_batch[\"input_ids\"].to(device)\n                    attention_mask = eval_batch[\"attention_mask\"].to(device)\n                    pixel_values = eval_batch[\"pixel_values\"].to(device)\n                    labels = eval_batch[\"labels\"].to(device)\n                    \n                    # Forward pass - FIXED: use base_model.forward to be consistent with training\n                    outputs = model.base_model.forward(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        pixel_values=pixel_values,\n                        labels=labels,\n                        return_dict=True\n                    )\n                    \n                    eval_loss += outputs.loss.item()\n                    eval_steps += 1\n            \n            avg_eval_loss = eval_loss / eval_steps\n            print(f\"Evaluation Loss: {avg_eval_loss:.4f}\")\n            \n            \n            # Generate example caption\n            if len(eval_dataset) > 0:\n                example = eval_dataset[0]\n                pixel_values = example[\"pixel_values\"].unsqueeze(0).to(device)\n                \n                # Generate with different parameters\n                with torch.no_grad():\n                    # Standard generation\n                    standard_ids = model.generate(\n                        pixel_values=pixel_values,\n                        max_new_tokens=50\n                    )\n                    standard_caption = processor.batch_decode(standard_ids, skip_special_tokens=True)[0]\n                    \n                    # Generation with sampling\n                    sampled_ids = model.generate(\n                        pixel_values=pixel_values,\n                        max_new_tokens=50,\n                        do_sample=True,\n                        top_k=50,\n                        top_p=0.9,\n                        temperature=0.7\n                    )\n                    sampled_caption = processor.batch_decode(sampled_ids, skip_special_tokens=True)[0]\n                \n                print(f\"Example standard caption: {standard_caption}\")\n                print(f\"Example sampled caption: {sampled_caption}\")\n                \n                # Get ground truth\n                example_input_ids = example[\"input_ids\"]\n                gt_caption = processor.tokenizer.decode(example_input_ids, skip_special_tokens=True)\n                print(f\"Ground truth caption: {gt_caption}\")\n            \n            # Save model checkpoint if it's the best so far\n            if avg_eval_loss < best_eval_loss:\n                best_eval_loss = avg_eval_loss\n                print(f\"New best model with eval loss: {best_eval_loss:.4f}\")\n                \n                # Save LoRA weights\n                output_dir = f\"./checkpoints/epoch-{epoch+1}_step-{global_step}_loss-{avg_eval_loss:.4f}\"\n                model.save_pretrained(output_dir)\n                print(f\"Model saved to {output_dir}\")\n            \n            # Back to training mode\n            model.train()\n\nprint(\"\\nTraining complete!\")\n\n# Final model saving\nmodel.save_pretrained(\"./fashion-captioning-final\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:08:11.968264Z","iopub.execute_input":"2025-04-05T09:08:11.968687Z","iopub.status.idle":"2025-04-05T10:05:37.006471Z","shell.execute_reply.started":"2025-04-05T09:08:11.968652Z","shell.execute_reply":"2025-04-05T10:05:37.005677Z"}},"outputs":[{"name":"stdout","text":"\nEpoch: 1/4\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"name":"stdout","text":"Step 10/1128, Loss: 43.4630, LR: 0.00002000\nStep 20/1128, Loss: 38.6788, LR: 0.00004000\nStep 30/1128, Loss: 25.4482, LR: 0.00006000\nStep 40/1128, Loss: 15.8447, LR: 0.00008000\nStep 50/1128, Loss: 11.9168, LR: 0.00010000\nStep 60/1128, Loss: 9.2244, LR: 0.00012000\nStep 70/1128, Loss: 7.4855, LR: 0.00014000\n\nEpoch: 2/4\nStep 80/1128, Loss: 6.1902, LR: 0.00016000\nStep 90/1128, Loss: 5.3047, LR: 0.00018000\nStep 100/1128, Loss: 4.7182, LR: 0.00020000\nStep 110/1128, Loss: 4.3414, LR: 0.00019995\nStep 120/1128, Loss: 3.9023, LR: 0.00019981\nStep 130/1128, Loss: 3.7749, LR: 0.00019958\nStep 140/1128, Loss: 3.6923, LR: 0.00019925\n\nEpoch: 3/4\nStep 150/1128, Loss: 3.5086, LR: 0.00019883\nStep 160/1128, Loss: 3.3550, LR: 0.00019832\nStep 170/1128, Loss: 3.1940, LR: 0.00019772\nStep 180/1128, Loss: 3.1261, LR: 0.00019703\nStep 190/1128, Loss: 3.0168, LR: 0.00019624\nStep 200/1128, Loss: 2.9336, LR: 0.00019537\n\nRunning evaluation...\nEvaluation Loss: 0.6928\nExample standard caption: solid dark blue long-sleeved jumper in a soft knit with a rounded neckline and ribbing around the neckline cuffs and hem\nExample sampled caption: solid dark blue jumper in a soft jersey with a v-neck and long sleeves with a button at the cuffs soft merino wool blend with a rounded neckline and ribbing around the neckline cuffs and hem\nGround truth caption: solid dark blue long-sleeved jumper in a soft fine knit containing some wool\nNew best model with eval loss: 0.6928\nModel saved to ./checkpoints/epoch-3_step-200_loss-0.6928\n\nRunning evaluation...\nEvaluation Loss: 0.6928\nExample standard caption: solid dark blue long-sleeved jumper in a soft knit with a rounded neckline and ribbing around the neckline cuffs and hem\nExample sampled caption: solid dark blue long-sleeved jumper in soft cashmere with a round neckline and ribbing at the cuffs\nGround truth caption: solid dark blue long-sleeved jumper in a soft fine knit containing some wool\n\nRunning evaluation...\nEvaluation Loss: 0.6928\nExample standard caption: solid dark blue long-sleeved jumper in a soft knit with a rounded neckline and ribbing around the neckline cuffs and hem\nExample sampled caption: solid dark blue long-sleeved top in a soft knit containing a small amount of wool with an opening and ribbing at the neckline cuffs and hem\nGround truth caption: solid dark blue long-sleeved jumper in a soft fine knit containing some wool\n\nRunning evaluation...\nEvaluation Loss: 0.6928\nExample standard caption: solid dark blue long-sleeved jumper in a soft knit with a rounded neckline and ribbing around the neckline cuffs and hem\nExample sampled caption: solid dark blue long-sleeved jumper in a soft knit with a rounded neckline and v-neck side pockets and ribbing around the neckline cuffs and hem\nGround truth caption: solid dark blue long-sleeved jumper in a soft fine knit containing some wool\nStep 210/1128, Loss: 2.9133, LR: 0.00019440\n\nEpoch: 4/4\nStep 220/1128, Loss: 2.9191, LR: 0.00019335\nStep 230/1128, Loss: 2.7555, LR: 0.00019221\nStep 240/1128, Loss: 2.6905, LR: 0.00019099\nStep 250/1128, Loss: 2.5194, LR: 0.00018968\nStep 260/1128, Loss: 2.6734, LR: 0.00018828\nStep 270/1128, Loss: 2.5970, LR: 0.00018681\nStep 280/1128, Loss: 2.5719, LR: 0.00018525\n\nTraining complete!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_caption(image_path, model, processor, max_new_tokens=50, do_sample=True):\n    # Load and process the image\n    image = Image.open(image_path).convert('RGB')\n    \n    # Process the image\n    inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n    \n    # Generate caption\n    with torch.no_grad():\n        # Standard generation\n        standard_ids = model.generate(\n            pixel_values=inputs.pixel_values,\n            max_new_tokens=max_new_tokens\n        )\n        standard_caption = processor.batch_decode(standard_ids, skip_special_tokens=True)[0]\n        \n        # Generation with sampling for more creative captions\n        if do_sample:\n            sampled_ids = model.generate(\n                pixel_values=inputs.pixel_values,\n                max_new_tokens=max_new_tokens,\n                do_sample=True,\n                top_k=50,\n                top_p=0.9,\n                temperature=0.7\n            )\n            sampled_caption = processor.batch_decode(sampled_ids, skip_special_tokens=True)[0]\n            return standard_caption, sampled_caption\n        \n        return standard_caption\n\n# 4. Test with an example image\nimage_path = \nstandard_caption, sampled_caption = generate_caption(image_path, model, processor)\n\nprint(f\"Standard caption: {standard_caption}\")\nprint(f\"Sampled caption: {sampled_caption}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First load the dataset as you've done\n\n\n# Modify your function to accept dataset examples instead of image paths\ndef generate_caption_from_dataset(dataset_example, model, processor, max_new_tokens=50, do_sample=True):\n    # Get the image from the dataset example\n    # First, check what fields are available\n    print(f\"Dataset example keys: {dataset_example.keys()}\")\n    \n    # Access the image - the field might be called 'image', 'pixel_values', or something else\n    # You'll need to adjust this based on the actual dataset structure\n    image = ds_small['image']  # Adjust this key as needed\n    \n    # Process the image\n    inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n    \n    # Generate caption\n    with torch.no_grad():\n        # Standard generation\n        standard_ids = model.generate(\n            pixel_values=inputs.pixel_values,\n            max_new_tokens=max_new_tokens\n        )\n        standard_caption = processor.batch_decode(standard_ids, skip_special_tokens=True)[0]\n        \n        # Generation with sampling for more creative captions\n        if do_sample:\n            sampled_ids = model.generate(\n                pixel_values=inputs.pixel_values,\n                max_new_tokens=max_new_tokens,\n                do_sample=True,\n                top_k=50,\n                top_p=0.9,\n                temperature=0.7\n            )\n            sampled_caption = processor.batch_decode(sampled_ids, skip_special_tokens=True)[0]\n            return standard_caption, sampled_caption\n        \n        return standard_caption\n\n# Test with a few examples from the dataset\nfor i in range(3):  # Try 3 examples\n    example = ds_small[i+500]\n    \n    # Generate captions\n    standard_caption, sampled_caption = generate_caption_from_dataset(example, model, processor)\n    \n    # Print results\n    print(f\"\\nImage {i}:\")\n    print(f\"Standard caption: {standard_caption}\")\n    print(f\"Sampled caption: {sampled_caption}\")\n    \n    # If the dataset has ground truth captions, print them for comparison\n    if 'caption' in example:\n        print(f\"Ground truth: {example['caption']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:26:54.615932Z","iopub.execute_input":"2025-04-05T10:26:54.616425Z","execution_failed":"2025-04-05T10:27:55.794Z"}},"outputs":[{"name":"stdout","text":"Dataset example keys: dict_keys(['text', 'image'])\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from transformers import Blip2Processor, Blip2ForConditionalGeneration, BitsAndBytesConfig\nfrom peft import PeftModel\nimport torch\nfrom PIL import Image\n\n# Path to your adapter files\nadapter_path = \"fashion-captioning-final\"\n\n# Set up quantization config\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\n# Load the processor from the base model\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n\n# Load the base model\nbase_model = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-opt-2.7b\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\n# Load your adapter weights onto the base model\nmodel = PeftModel.from_pretrained(base_model, adapter_path)\nmodel.eval()\n\n# Path to your test image\nimage_path = ds_small['image'][588]  # Replace with actual path\n\n# Load and process the image\nimage = Image.open(image_path).convert('RGB')\ninputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n\n# Generate caption\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=50,\n        num_beams=5,\n    )\n\n# Decode the caption\ncaption = processor.decode(outputs[0], skip_special_tokens=True)\nprint(f\"Image: {image_path}\")\nprint(f\"Caption: {caption}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:43:01.948481Z","iopub.execute_input":"2025-04-05T10:43:01.948939Z","iopub.status.idle":"2025-04-05T10:45:13.622062Z","shell.execute_reply.started":"2025-04-05T10:43:01.948910Z","shell.execute_reply":"2025-04-05T10:45:13.620736Z"}},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d6115898ab64d0488cdb176094ed5ff"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.language_model.model.decoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.27.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.27.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.28.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.28.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.29.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.29.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.30.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.30.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.31.self_attn.k_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.31.self_attn.k_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.language_model.model.decoder.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.language_model.model.decoder.layers.31.self_attn.q_proj.lora_B.default.weight'].\n  warnings.warn(warn_message)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-bd2d5e47cba0>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Load and process the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3480\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3482\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: read"],"ename":"AttributeError","evalue":"read","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"from transformers import Blip2Processor, Blip2ForConditionalGeneration, BitsAndBytesConfig\nfrom peft import PeftModel\nimport torch\nimport gc\n\n# Path to your adapter files\nadapter_path = \"fashion-captioning-final\"\n\n# Set up quantization config\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\n# Load the processor from the base model\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n\n# Load the base model\nbase_model = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-opt-2.7b\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\n# Load your adapter weights onto the base model\nmodel = PeftModel.from_pretrained(base_model, adapter_path)\nmodel.eval()\n\n# Get the image directly from your dataset\nimage = ds_small['image'][0]\n\n# Check what type the image is and process accordingly\nif hasattr(image, 'convert'):  # It's a PIL Image\n    # Just use it directly\n    pass\nelif isinstance(image, torch.Tensor):  # It's a tensor\n    # No need to use Image.open, just use the tensor\n    pass\nelse:\n    # If it's a numpy array, convert to tensor or PIL Image\n    from PIL import Image\n    import numpy as np\n    if isinstance(image, np.ndarray):\n        image = Image.fromarray(image).convert('RGB')\n\n# Process the image\ninputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n\n# Generate caption\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=50,\n        num_beams=5,\n    )\n\n# Clear some memory\ndel inputs\ntorch.cuda.empty_cache()\ngc.collect()\n\n# Decode the caption\ncaption = processor.decode(outputs[0], skip_special_tokens=True)\nprint(f\"Caption: {caption}\")\n\n# More memory cleanup\ndel outputs\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:46:53.009280Z","iopub.execute_input":"2025-04-05T10:46:53.009726Z","iopub.status.idle":"2025-04-05T10:49:14.289675Z","shell.execute_reply.started":"2025-04-05T10:46:53.009692Z","shell.execute_reply":"2025-04-05T10:49:14.288761Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"876f8025b37845bd955ed26d06f8040e"}},"metadata":{}},{"name":"stdout","text":"Caption: long-sleeve t-shirt - charcoal\n\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def generate_caption(image, model, processor, max_new_tokens=50, do_sample=True):\n    \"\"\"\n    Generate captions for an image\n    image: Can be either a file path or a direct image object from dataset\n    \"\"\"\n    # Check if input is a path string or already an image\n    if isinstance(image, str):\n        # It's a file path, open the image\n        image = Image.open(image).convert('RGB')\n    else:\n        # It's already an image object from dataset\n        # Check if it needs conversion\n        if hasattr(image, 'convert'):\n            # It's a PIL Image, just ensure it's RGB\n            image = image.convert('RGB')\n        elif isinstance(image, torch.Tensor):\n            # If it's a tensor, no need for Image.open\n            pass\n        else:\n            # If it's a numpy array, convert to PIL Image\n            import numpy as np\n            if isinstance(image, np.ndarray):\n                image = Image.fromarray(image).convert('RGB')\n    \n    # Process the image\n    inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n    \n    # Generate caption\n    with torch.no_grad():\n        # Standard generation\n        standard_ids = model.generate(\n            pixel_values=inputs.pixel_values,\n            max_new_tokens=max_new_tokens\n        )\n        standard_caption = processor.batch_decode(standard_ids, skip_special_tokens=True)[0]\n        \n        # Generation with sampling for more creative captions\n        if do_sample:\n            sampled_ids = model.generate(\n                pixel_values=inputs.pixel_values,\n                max_new_tokens=max_new_tokens,\n                do_sample=True,\n                top_k=50,\n                top_p=0.9,\n                temperature=0.7\n            )\n            sampled_caption = processor.batch_decode(sampled_ids, skip_special_tokens=True)[0]\n            \n            # Clean up memory\n            del inputs, standard_ids, sampled_ids\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n            return standard_caption, sampled_caption\n        \n        # Clean up memory\n        del inputs, standard_ids\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        return standard_caption\n\n# Now use it with your dataset image\nimage_obj = ds_small['image'][500]  # Get image from dataset\nstandard_caption, sampled_caption = generate_caption(image_obj, model, processor)\nprint(f\"Standard caption: {standard_caption}\")\nprint(f\"Sampled caption: {sampled_caption}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T11:03:50.995921Z","iopub.execute_input":"2025-04-05T11:03:50.996515Z","iopub.status.idle":"2025-04-05T11:04:37.680478Z","shell.execute_reply.started":"2025-04-05T11:03:50.996457Z","shell.execute_reply":"2025-04-05T11:04:37.679489Z"}},"outputs":[{"name":"stdout","text":"Standard caption: a white shirt with a button down collar\n\nSampled caption: the button down shirt in white\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"ds_small['text'][500] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T11:08:32.185070Z","iopub.execute_input":"2025-04-05T11:08:32.185478Z","iopub.status.idle":"2025-04-05T11:08:32.194825Z","shell.execute_reply.started":"2025-04-05T11:08:32.185452Z","shell.execute_reply":"2025-04-05T11:08:32.193967Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'solid white straight-cut shirt in a cotton weave with a collar chest pocket concealed buttons down the front and long sleeves with buttoned cuffs'"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"def generate_caption(image, model, processor, max_new_tokens=50, do_sample=True):\n    \"\"\"\n    Generate captions for an image - handles both file paths and dataset images\n    \"\"\"\n    try:\n        # Process the image based on its type\n        if isinstance(image, str):\n            # It's a file path\n            image = Image.open(image).convert('RGB')\n            pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(model.device)\n            \n        elif hasattr(image, 'convert'):\n            # It's a PIL Image\n            pixel_values = processor(images=image.convert('RGB'), return_tensors=\"pt\").pixel_values.to(model.device)\n            \n        elif isinstance(image, torch.Tensor):\n            # It's already a tensor, check if it needs reshaping\n            if len(image.shape) == 3:  # [C, H, W]\n                # Add batch dimension if needed\n                pixel_values = image.unsqueeze(0).to(model.device)\n            else:\n                # Assume it's already properly formatted\n                pixel_values = image.to(model.device)\n                \n        else:\n            # Try to convert from numpy array\n            import numpy as np\n            if isinstance(image, np.ndarray):\n                image = Image.fromarray(image).convert('RGB')\n                pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(model.device)\n            else:\n                raise TypeError(f\"Unsupported image type: {type(image)}\")\n        \n        # Generate caption\n        with torch.no_grad():\n            # Standard generation - match your training code exactly\n            standard_ids = model.generate(\n                pixel_values=pixel_values,\n                max_new_tokens=max_new_tokens\n            )\n            standard_caption = processor.batch_decode(standard_ids, skip_special_tokens=True)[0]\n            \n            # Generation with sampling for more creative captions\n            if do_sample:\n                sampled_ids = model.generate(\n                    pixel_values=pixel_values,\n                    max_new_tokens=max_new_tokens,\n                    do_sample=True,\n                    top_k=50,\n                    top_p=0.9,\n                    temperature=0.7\n                )\n                sampled_caption = processor.batch_decode(sampled_ids, skip_special_tokens=True)[0]\n                \n                # Clean up memory\n                del pixel_values, standard_ids, sampled_ids\n                torch.cuda.empty_cache()\n                gc.collect()\n                \n                return standard_caption, sampled_caption\n            \n            # Clean up memory\n            del pixel_values, standard_ids\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n            return standard_caption\n            \n    except Exception as e:\n        print(f\"Error generating caption: {e}\")\n        import traceback\n        traceback.print_exc()\n        return \"Error generating caption\", \"Error generating caption\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T11:12:50.069636Z","iopub.execute_input":"2025-04-05T11:12:50.070069Z","iopub.status.idle":"2025-04-05T11:12:50.079535Z","shell.execute_reply.started":"2025-04-05T11:12:50.070040Z","shell.execute_reply":"2025-04-05T11:12:50.078372Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Get an image from your dataset\nimage_obj = ds_small['image'][500]\n\n# Generate captions\nstandard_caption, sampled_caption = generate_caption(image_obj, model, processor)\n\nprint(f\"Standard caption: {standard_caption}\")\nprint(f\"Sampled caption: {sampled_caption}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T11:13:07.486744Z","iopub.execute_input":"2025-04-05T11:13:07.487099Z","iopub.status.idle":"2025-04-05T11:13:45.518496Z","shell.execute_reply.started":"2025-04-05T11:13:07.487075Z","shell.execute_reply":"2025-04-05T11:13:45.517711Z"}},"outputs":[{"name":"stdout","text":"Standard caption: a white shirt with a button down collar\n\nSampled caption: a white shirt with pockets\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}